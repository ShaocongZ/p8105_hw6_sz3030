---
title: "p8105_hw6_sz3030"
author: "Shaocong Zhang"
date: "12/4/2021"
output: github_document
---

# Problem 1

```{r}
library(tidyverse)
library(modelr)
library(leaps)
library(mgcv)
```



### Load the dataset

```{r}
birthweight = read_csv("./data/birthweight.csv")
```

### Clean the data

```{r}
birthweight = 
  birthweight %>% 
  janitor::clean_names() %>%
  mutate(
    babysex = as.factor(babysex),
    babysex = fct_recode(babysex, "male" = "1", 
                         "female" = "2"),
    frace = as.factor(frace),
    frace = fct_recode(frace, "white" = "1", 
                       "black" = "2", 
                       "asian" = "3", 
                       "puerto rican" = "4", 
                       "other" = "8", 
                       "unknown" = "9"),
    malform = as.factor(malform),
    malform = fct_recode(malform, "absent" = "0", 
                         "present" = "1"),
    mrace = as.factor(mrace),
    mrace = fct_recode(mrace, "white" = "1", 
                       "black" = "2", 
                       "asian" = "3", 
                       "puerto rican" = "4"))
```


### Check for missing data

```{r}
sum(is.na(birthweight))
```

There is 0 NA value in the dataset.

### model selection

```{r}
full.mod = lm(bwt  ~., data = birthweight)

step(full.mod, direction = 'backward', trace = FALSE)
```

In the model selection process, I use the backward selection method, which start with the full model and exclude the covariates to decrease the AIC value.

```{r}
# Thus, the final model should be
final.mod = lm(formula = bwt ~ babysex + bhead + blength + delwt + fincome + 
    gaweeks + mheight + mrace + parity + ppwt + smoken, data = birthweight)

birthweight %>%
  add_predictions(final.mod) %>%
  add_residuals(final.mod) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(
    title = "Residuals v.s. fitted values",
    x = "Fitted values",
    y = "Residuals"
  )
```

### Compare the model to two others

```{r}
set.seed(1)

cv_df = 
  crossv_mc(birthweight, 100) %>% 
  mutate(
    maineffect_mod = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    final_mod = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + 
                         mheight + mrace + parity + ppwt + smoken, data = .x)),
    complex_mod =  map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_maineffect = map2_dbl(maineffect_mod, test, rmse),
    rmse_final = map2_dbl(final_mod, test, rmse),
    rmse_complex = map2_dbl(complex_mod, test, rmse)) 

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

It is obvious that the final model, which was selected after the backward selection, performs best among all the three model. The maineffect model, which only contains length at birth and gestational age as predictors, performs worst due to its highest RMSE. The complex model, which contains head circumference, length, sex, and all interactions, performs good. However, due to its complexity in model computing and interpretation, I would not try to use it. The "final model" would be the best choice.

# Problem 2


